# -*- coding: utf-8 -*-
"""05_advanced_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E5FMbtsatVb9QDxeigux3r23Xac-0WkR
"""

# Commented out IPython magic to ensure Python compatibility.
# mount & navigate
from google.colab import drive
drive.mount('/content/drive')
!git clone https://github.com/urverse/Visualizing-the-T.git
# %cd Visualizing-the-T

# install
!pip install pyspark scipy -q

# imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

print("‚úÖ Setup complete")

# load all data
rss_results = pd.read_csv('data/processed/rss_final_results.csv')
travel_indicators = pd.read_parquet("data/indicators/travel_reliability_indicators.parquet")
restriction_indicators = pd.read_parquet("data/indicators/restriction_indicators.parquet")
ridership_weights = pd.read_parquet("data/indicators/ridership_weights.parquet")

print("=== DATA LOADED ===")
print(f"Travel indicators: {len(travel_indicators):,} segments")
print(f"Routes in RSS: {rss_results['route_id'].tolist()}")

print("="*70)
print("STATION-LEVEL RSS ANALYSIS")
print("="*70)

# aggregate indicators by station (from_stop_id)
station_indicators = travel_indicators.groupby(['route_id', 'from_stop_id']).agg({
    'median_travel_time': 'mean',
    'travel_time_volatility': 'mean',
    'buffer_time_index': 'mean',
    'on_time_performance': 'mean',
    'n_observations': 'sum'
}).reset_index()

print(f"\n=== STATION-LEVEL INDICATORS ===")
print(f"Total stations analyzed: {len(station_indicators)}")
print(f"\nSample:")
print(station_indicators.head(10))

# normalize indicators
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

indicators_to_scale = ['median_travel_time', 'travel_time_volatility', 'buffer_time_index', 'on_time_performance']
station_indicators[indicators_to_scale] = scaler.fit_transform(station_indicators[indicators_to_scale])

# compute station-level RSS (using same weights)
weights_lit = {'median_travel_time': -0.15, 'travel_time_volatility': -0.25,
               'buffer_time_index': -0.25, 'on_time_performance': 0.25}

station_indicators['station_rss'] = sum(
    weights_lit[col] * station_indicators[col] for col in weights_lit.keys()
)

# scale to 60-100
rss_min = station_indicators['station_rss'].min()
rss_max = station_indicators['station_rss'].max()
station_indicators['station_rss_scaled'] = 60 + 40 * (
    station_indicators['station_rss'] - rss_min
) / (rss_max - rss_min)

print("\n=== TOP 10 BEST PERFORMING STATIONS ===")
print(station_indicators.nlargest(10, 'station_rss_scaled')[['route_id', 'from_stop_id', 'station_rss_scaled']])

print("\n=== TOP 10 WORST PERFORMING STATIONS ===")
print(station_indicators.nsmallest(10, 'station_rss_scaled')[['route_id', 'from_stop_id', 'station_rss_scaled']])

# show station performance distribution by route
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

color_map = {'Blue': '#003DA5', 'Orange': '#ED8B00', 'Red': '#DA291C'}

# 1. distribution of station scores by route
for route in ['Blue', 'Orange', 'Red']:
    route_data = station_indicators[station_indicators['route_id'] == route]['station_rss_scaled']
    axes[0,0].hist(route_data, bins=15, alpha=0.6, label=route, color=color_map[route])

axes[0,0].set_xlabel('Station RSS Score')
axes[0,0].set_ylabel('Number of Stations')
axes[0,0].set_title('Distribution of Station Scores by Route', fontsize=14, fontweight='bold')
axes[0,0].legend()
axes[0,0].axvline(x=80, color='black', linestyle='--', linewidth=2, label='Target')

# 2. box plot comparison
station_data_list = []
for route in ['Blue', 'Orange', 'Red']:
    route_data = station_indicators[station_indicators['route_id'] == route]
    for _, row in route_data.iterrows():
        station_data_list.append({'Route': route, 'RSS': row['station_rss_scaled']})

station_df = pd.DataFrame(station_data_list)
sns.boxplot(data=station_df, x='Route', y='RSS', ax=axes[0,1], palette=color_map)
axes[0,1].set_title('Station RSS Distribution by Route', fontsize=14, fontweight='bold')
axes[0,1].set_ylabel('Station RSS (60-100)')
axes[0,1].axhline(y=80, color='gray', linestyle='--', alpha=0.5)

# 3. worst performing stations across all routes
worst_stations = station_indicators.nsmallest(10, 'station_rss_scaled')
colors_worst = [color_map[route] for route in worst_stations['route_id']]
axes[1,0].barh(range(len(worst_stations)), worst_stations['station_rss_scaled'], color=colors_worst)
axes[1,0].set_yticks(range(len(worst_stations)))
axes[1,0].set_yticklabels([f"{row['route_id']}-{row['from_stop_id']}" for _, row in worst_stations.iterrows()], fontsize=9)
axes[1,0].set_xlabel('RSS Score')
axes[1,0].set_title('Top 10 Worst Performing Stations', fontsize=14, fontweight='bold')
axes[1,0].set_xlim(55, 85)

# 4. best performing stations
best_stations = station_indicators.nlargest(10, 'station_rss_scaled')
colors_best = [color_map[route] for route in best_stations['route_id']]
axes[1,1].barh(range(len(best_stations)), best_stations['station_rss_scaled'], color=colors_best)
axes[1,1].set_yticks(range(len(best_stations)))
axes[1,1].set_yticklabels([f"{row['route_id']}-{row['from_stop_id']}" for _, row in best_stations.iterrows()], fontsize=9)
axes[1,1].set_xlabel('RSS Score')
axes[1,1].set_title('Top 10 Best Performing Stations', fontsize=14, fontweight='bold')
axes[1,1].set_xlim(85, 105)

plt.tight_layout()
plt.show()

print("‚úÖ Station performance summary complete")

# save station results
station_indicators.to_csv('data/processed/station_rss_scores.csv', index=False)
print("‚úÖ Saved: station_rss_scores.csv")

print("="*70)
print("BOOTSTRAP CONFIDENCE INTERVALS - RESAMPLING")
print("="*70)

# bootstrap function
def bootstrap_rss(data, weights, n_bootstrap=1000):
    bootstrap_scores = []

    for i in range(n_bootstrap):
        # resample with replacement
        sample = data.sample(n=len(data), replace=True)

        # normalize
        scaler = StandardScaler()
        sample_norm = sample.copy()
        indicators = ['median_travel_time', 'travel_time_volatility', 'buffer_time_index', 'on_time_performance']
        sample_norm[indicators] = scaler.fit_transform(sample[indicators])

        # compute RSS
        weights = {'median_travel_time': -0.15, 'travel_time_volatility': -0.25,
                   'buffer_time_index': -0.25, 'on_time_performance': 0.25}
        sample_norm['rss'] = sum(weights[col] * sample_norm[col] for col in weights.keys())

        # scale to 60-100
        rss_min = sample_norm['rss'].min()
        rss_max = sample_norm['rss'].max()
        sample_norm['rss_scaled'] = 60 + 40 * (sample_norm['rss'] - rss_min) / (rss_max - rss_min)
        w = weights.loc[sample_norm.index]
        bootstrap_scores.append(np.average(sample_norm['rss_scaled'], weights=w))

    return np.array(bootstrap_scores)

# compute bootstrap CIs for each route
print("\n=== COMPUTING BOOTSTRAP CONFIDENCE INTERVALS ===")
print("(1000 resamples per route)\n")

ci_results = []

travel_times_sample = pd.read_csv('data/processed/travel_times_sample.csv')
ridership_sample = pd.read_csv('data/processed/ridership_sample.csv')
mapping = travel_times_sample[['from_stop_id', 'from_stop_name']].drop_duplicates().dropna()
mapping['from_stop_id'] = mapping['from_stop_id'].astype(str)
station_indicators['from_stop_id'] = station_indicators['from_stop_id'].astype(str)
station_indicators = station_indicators.merge(mapping, on='from_stop_id', how='left')
ridership_weights_df = ridership_sample.groupby(['route_id', 'stop_name'])['average_flow'].sum().reset_index()
ridership_weights_df.rename(columns={'stop_name': 'from_stop_name', 'average_flow': 'ridership_weight'}, inplace=True)

ci_results = []

for route in ['Blue', 'Orange', 'Red']:
    route_data = station_indicators[station_indicators['route_id'] == route].copy()
    route_data = route_data.merge(ridership_weights_df, on=['route_id', 'from_stop_name'], how='left')
    route_data['ridership_weight'] = route_data['ridership_weight'].fillna(1.0)
    route_data.reset_index(drop=True, inplace=True)
    weights_series = route_data['ridership_weight']

    bootstrap_samples = bootstrap_rss(route_data, weights_series, n_bootstrap=1000)

    # compute 95% CI
    ci_lower = np.percentile(bootstrap_samples, 2.5)
    ci_upper = np.percentile(bootstrap_samples, 97.5)
    mean_score = np.mean(bootstrap_samples)

    ci_results.append({
        'Route': route,
        'Mean_RSS': mean_score,
        'CI_Lower': ci_lower,
        'CI_Upper': ci_upper,
        'CI_Width': ci_upper - ci_lower
    })

    print(f"{route} Line:")
    print(f"  Mean RSS: {mean_score:.2f}")
    print(f"  95% CI: [{ci_lower:.2f}, {ci_upper:.2f}]")
    print(f"  CI Width: {ci_upper - ci_lower:.2f}\n")

ci_df = pd.DataFrame(ci_results)
print("‚úÖ Bootstrap analysis complete")

# visualize bootstrap confidence intervals
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# 1. RSS with confidence intervals
color_map = {'Blue': '#003DA5', 'Orange': '#ED8B00', 'Red': '#DA291C'}
colors = [color_map[route] for route in ci_df['Route']]

axes[0].barh(ci_df['Route'], ci_df['Mean_RSS'], color=colors, alpha=0.7)
axes[0].errorbar(ci_df['Mean_RSS'], ci_df['Route'],
                 xerr=[(ci_df['Mean_RSS'] - ci_df['CI_Lower']).values,
                       (ci_df['CI_Upper'] - ci_df['Mean_RSS']).values],
                 fmt='none', ecolor='black', capsize=5, linewidth=2)

for i, row in ci_df.iterrows():
    axes[0].text(row['Mean_RSS']+1, i, f"{row['Mean_RSS']:.1f}",
                va='center', fontweight='bold')

axes[0].set_xlabel('Station-Level RSS (60-100)')
axes[0].set_title('Route RSS with 95% Confidence Intervals\n(Bootstrap, n=1000)',
                  fontsize=14, fontweight='bold')
axes[0].set_xlim(70, 100)
axes[0].grid(axis='x', alpha=0.3)

# 2. CI width comparison (uncertainty)
axes[1].bar(ci_df['Route'], ci_df['CI_Width'], color=colors, alpha=0.7)
axes[1].set_ylabel('Confidence Interval Width')
axes[1].set_xlabel('Route')
axes[1].set_title('RSS Uncertainty by Route\n(Narrower = More Consistent)',
                  fontsize=14, fontweight='bold')
axes[1].set_ylim(0, 15)

for i, row in ci_df.iterrows():
    axes[1].text(i, row['CI_Width']+0.3, f"{row['CI_Width']:.1f}",
                ha='center', fontweight='bold')

plt.tight_layout()
plt.show()

print("\n=== INTERPRETATION ===")
print("‚Ä¢ Orange Line: Highest mean RSS (88.76) with narrowest CI (11.89)")
print("  ‚Üí Most consistent performance across stations")
print("‚Ä¢ Blue Line: Lower mean (85.64) with widest CI (13.24)")
print("  ‚Üí Most variable performance, some stations much worse")
print("‚Ä¢ Red Line: Middle mean (86.53) with moderate CI (12.59)")

# save results
ci_df.to_csv('data/processed/bootstrap_confidence_intervals.csv', index=False)
print("\n‚úÖ Saved: bootstrap_confidence_intervals.csv")

from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import cross_val_score

print("="*70)
print("PREDICTIVE MODEL - REGRESSION TO LEARN OPTIMAL WEIGHTS")
print("="*70)

# prepare data for regression
# target: station RSS (already computed)
# features: normalized indicators

X = station_indicators[['median_travel_time', 'travel_time_volatility',
                        'buffer_time_index', 'on_time_performance']].copy()
y = station_indicators['station_rss_scaled'].copy()

print(f"\n=== DATA PREPARATION ===")
print(f"Features: {X.columns.tolist()}")
print(f"Target: Station RSS (60-100)")
print(f"Samples: {len(X)}")

# train ridge regression model
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X, y)

# predictions
y_pred = ridge_model.predict(X)

# evaluate
r2 = r2_score(y, y_pred)
rmse = np.sqrt(mean_squared_error(y, y_pred))

# cross-validation
cv_scores = cross_val_score(ridge_model, X, y, cv=5, scoring='r2')

print(f"\n=== MODEL PERFORMANCE ===")
print(f"R¬≤ Score: {r2:.4f}")
print(f"RMSE: {rmse:.2f}")
print(f"Cross-Validation R¬≤ (5-fold): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

# extract learned weights
learned_weights = pd.DataFrame({
    'Feature': X.columns,
    'Weight': ridge_model.coef_
}).sort_values('Weight', ascending=False)

print(f"\n=== LEARNED WEIGHTS (Ridge Regression) ===")
for _, row in learned_weights.iterrows():
    print(f"{row['Feature']:.<30} {row['Weight']:>8.4f}")

print(f"\nIntercept: {ridge_model.intercept_:.4f}")

# compare with literature weights
lit_weights = pd.DataFrame({
    'Feature': ['travel_time_volatility', 'buffer_time_index',
                'on_time_performance', 'median_travel_time'],
    'Literature_Weight': [-0.25, -0.25, 0.25, -0.15]
})

weight_comparison = learned_weights.merge(lit_weights, on='Feature')
print(f"\n=== WEIGHT COMPARISON ===")
print(weight_comparison)

print("\n‚úÖ Predictive model complete")

# visualize model performance
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. predicted vs actual
axes[0,0].scatter(y, y_pred, alpha=0.5, s=30)
axes[0,0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)
axes[0,0].set_xlabel('Actual RSS')
axes[0,0].set_ylabel('Predicted RSS')
axes[0,0].set_title(f'Predicted vs Actual RSS\n(R¬≤ = {r2:.4f})', fontsize=14, fontweight='bold')
axes[0,0].grid(alpha=0.3)

# 2. residuals
residuals = y - y_pred
axes[0,1].scatter(y_pred, residuals, alpha=0.5, s=30)
axes[0,1].axhline(y=0, color='r', linestyle='--', linewidth=2)
axes[0,1].set_xlabel('Predicted RSS')
axes[0,1].set_ylabel('Residuals')
axes[0,1].set_title('Residual Plot', fontsize=14, fontweight='bold')
axes[0,1].grid(alpha=0.3)

# 3. feature importance (absolute weights)
feature_importance = learned_weights.copy()
feature_importance['Abs_Weight'] = feature_importance['Weight'].abs()
feature_importance = feature_importance.sort_values('Abs_Weight', ascending=True)

colors_feat = ['green' if w > 0 else 'red' for w in feature_importance['Weight']]
axes[1,0].barh(feature_importance['Feature'], feature_importance['Abs_Weight'], color=colors_feat, alpha=0.7)
axes[1,0].set_xlabel('Absolute Weight (Importance)')
axes[1,0].set_title('Feature Importance\n(Green=Positive, Red=Negative)', fontsize=14, fontweight='bold')
axes[1,0].grid(axis='x', alpha=0.3)

# 4. literature vs learned weights comparison
weight_comp = weight_comparison.set_index('Feature')
weight_comp[['Literature_Weight', 'Weight']].plot(kind='barh', ax=axes[1,1], width=0.8)
axes[1,1].set_xlabel('Weight Value')
axes[1,1].set_title('Literature vs Learned Weights', fontsize=14, fontweight='bold')
axes[1,1].legend(['Literature', 'Learned (Ridge)'])
axes[1,1].grid(axis='x', alpha=0.3)
axes[1,1].axvline(x=0, color='black', linewidth=1)

plt.tight_layout()
plt.show()

print("‚úÖ Model visualization complete")

# save model results
model_results = pd.DataFrame({
    'Metric': ['R¬≤ Score', 'RMSE', 'CV R¬≤ Mean', 'CV R¬≤ Std'],
    'Value': [r2, rmse, cv_scores.mean(), cv_scores.std()]
})
model_results.to_csv('data/processed/regression_model_results.csv', index=False)
learned_weights.to_csv('data/processed/learned_weights.csv', index=False)

print("‚úÖ Saved: regression_model_results.csv, learned_weights.csv")

print("="*70)
print("FINAL PROJECT SUMMARY - COMPLETE ANALYSIS")
print("="*70)

print("\nüìä DELIVERABLES COMPLETED:")
print("-" * 70)
print("‚úÖ Week 1: Data consolidation (Eric)")
print("‚úÖ Week 2: RSS calculation with literature-based weights")
print("‚úÖ Week 3: Advanced analytics")
print("   ‚Ä¢ Hypothesis testing (ANOVA)")
print("   ‚Ä¢ Station-level breakdown (111 stations)")
print("   ‚Ä¢ Bootstrap confidence intervals (1000 resamples)")
print("   ‚Ä¢ Predictive regression model (R¬≤=1.0)")
print("   ‚Ä¢ Equity analysis (income + race)")

print("\nüéØ KEY FINDINGS:")
print("-" * 70)
print("1. RSS SCORES (Literature-Based):")
print("   ‚Ä¢ Orange: 100.0 (best overall)")
print("   ‚Ä¢ Red: 68.0")
print("   ‚Ä¢ Blue: 60.0")

print("\n2. STATION-LEVEL ANALYSIS:")
print("   ‚Ä¢ 111 stations analyzed")
print("   ‚Ä¢ Best: Red-70085 (100.0)")
print("   ‚Ä¢ Worst: Orange-70007 (60.0)")
print("   ‚Ä¢ Orange most consistent (CI width: 11.89)")

print("\n3. HYPOTHESIS TESTING:")
print("   ‚Ä¢ Volatility: Significantly different (p=0.018)")
print("   ‚Ä¢ On-Time Performance: Not significant (p=0.229)")

print("\n4. EQUITY ANALYSIS:")
print("   ‚Ä¢ NO DISCRIMINATION: Orange (best RSS) serves:")
print("     - 61% low-income riders")
print("     - 66% minority riders")
print("   ‚Ä¢ Positive correlation (r=1.0)")

print("\n5. PREDICTIVE MODEL:")
print("   ‚Ä¢ Ridge Regression: R¬≤=1.0, RMSE=0.04")
print("   ‚Ä¢ Most important: Buffer time & volatility")
print("   ‚Ä¢ Learned weights align with literature")

print("\nüìÅ FILES GENERATED:")
print("-" * 70)
files_list = [
    'rss_final_results.csv',
    'rss_weights.csv',
    'station_rss_scores.csv',
    'bootstrap_confidence_intervals.csv',
    'regression_model_results.csv',
    'learned_weights.csv'
]
for f in files_list:
    print(f"  ‚Ä¢ {f}")

print("\nüé® VISUALIZATIONS CREATED:")
print("-" * 70)
viz_list = [
    '1. RSS comparison (equal vs literature weights)',
    '2. Hypothesis testing (volatility & OTP)',
    '3. Equity analysis (4 charts)',
    '4. Station performance (distribution + top/bottom)',
    '5. Bootstrap confidence intervals',
    '6. Predictive model (4 charts)'
]
for v in viz_list:
    print(f"  {v}")

print("\n‚öôÔ∏è TECHNOLOGIES USED:")
print("-" * 70)
print("  ‚Ä¢ PySpark: Data loading & initial processing")
print("  ‚Ä¢ Pandas: Data manipulation & analysis")
print("  ‚Ä¢ Seaborn/Matplotlib: Visualizations")
print("  ‚Ä¢ Scikit-learn: Regression modeling")
print("  ‚Ä¢ SciPy: Hypothesis testing (ANOVA, t-tests)")
print("  ‚Ä¢ Bootstrap resampling: Confidence intervals")

print("\nüöÄ READY FOR PRESENTATION (DEC 1)")
print("="*70)
